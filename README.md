# Text-Classification-using-distill-bert-model

A project that demonstrates the power of transformer-based models for binary text classification using **DistilBERT**, fine-tuned on a custom dataset and deployed via an interactive **Streamlit** web app.

---

## ğŸ“Œ Project Overview

This project builds a lightweight yet powerful **text classification system** using the **DistilBERT** model from Hugging Face Transformers. The model classifies input text into two categories (e.g., spam vs. not spam, positive vs. negative). It is fine-tuned on a labeled dataset using PyTorch and deployed with Streamlit for real-time user interaction.

---

## ğŸ› ï¸ Tech Stack / Tools Used

- ğŸ”¤ **NLP & Transformers**: Hugging Face `transformers`, `DistilBERT`
- âš™ï¸ **Model Training**: PyTorch, scikit-learn
- ğŸ“Š **Evaluation Metrics**: Accuracy, Precision, Recall, F1-Score
- ğŸ§ª **Dataset Handling**: pandas, Excel
- ğŸ’» **Deployment**: Streamlit (Python Web App Framework)
- ğŸ“ˆ **Progress Monitoring**: tqdm
- ğŸ’¾ **Model Saving**: `model.save_pretrained()`

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ app.py                  # Streamlit web app
â”œâ”€â”€ train_model.ipynb       # Model training notebook
â”œâ”€â”€ distilbert_model/       # Saved model and tokenizer
â”œâ”€â”€ output_dataset.xlsx     # Labeled dataset for training
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md               # Project documentation
```

---

## ğŸš€ How to Run the Project Locally

### ğŸ”§ Step 1: Clone the Repository
```bash
git clone https://github.com/your-username/distilbert-text-classification.git
cd distilbert-text-classification

## ğŸ“¦ Step 1: Clone the Repository

```bash
git clone https://github.com/your-username/text-classification-distilbert.git
cd text-classification-distilbert
```

## ğŸ“¦ Step 2: Install Dependencies

```bash
pip install -r requirements.txt
```

## ğŸ¯ Step 3: Train the Model (Optional)

```bash
jupyter notebook train_model.ipynb
```

> If you don't want to train the model, you can use the pre-trained one available in the repo.

## ğŸ–¥ï¸ Step 4: Run the Streamlit App

```bash
streamlit run app.py
```

---

## ğŸ§ª Sample Output

- **Input Text:** `I loved the service!`  
- **Predicted Label:** `Positive (Label = 1)`

---

## ğŸ“Š Model Performance

| Metric     | Class 0 | Class 1 |
|------------|---------|---------|
| Precision  | 0.91    | 0.88    |
| Recall     | 0.89    | 0.91    |
| F1-Score   | 0.90    | 0.89    |

- **Overall Accuracy:** ~90%

---

## âœ… Features

- Fine-tuned **DistilBERT** on binary classification dataset
- Fast tokenization with `DistilBertTokenizerFast`
- **PyTorch** custom dataset class and training loop
- Interactive predictions via **Streamlit** UI
- Saved model for easy deployment and reuse

---

## ğŸ§© Future Enhancements

- âœ… Multi-class classification
- âœ… File upload support for batch predictions
- âœ… Backend API using **FastAPI**
- âœ… Deployment on **Hugging Face Spaces** or **Streamlit Cloud**

---

## ğŸ§  Skills Gained

`NLP`, `Transformer Models`, `Hugging Face`, `PyTorch`, `Text Classification`,  
`Fine-tuning`, `Model Deployment`, `Streamlit`, `Data Preprocessing`

---

## ğŸ“œ License

This project is licensed under the **MIT License**.  
Feel free to **use, modify, and distribute** for personal or commercial use.
